---
title: "Exam 3 Solutions"
author: "Ben White"
date: "4/19/2018"
output:
  pdf_document: default
  html_document: default
---

1.
    a. (1 point) This is a Bernoulli population, so the first moment is $E[X_i] = p$: $$E[X_i] = \sum xf(x) = 0(1-p) + 1(p) = p.$$ Matching $p$ to the first sample moment (the sample mean) we get $\bar{X} = \frac{1}{n}\sum X_i$ as our moment estimator of $p$.
    
    b. (1 point) Likelihood function: $$L(p|x_1,...,x_n) = \prod f(x_i|p) = \prod p^{x_i}(1-p)^{n-x_i} = p^{\sum x_i}(1-p)^{n-\sum x_i}$$ Log likelihood: $$\log L(p) = \log\left(p^{\sum x_i}(1-p)^{n-\sum x_i}\right) = \log p (\sum x_i) + \log(1-p)(n - \sum x_i)$$ Maximizing: $$0 = \frac{d}{dp} \log L(p)$$ $$= \frac{\sum x_i}{p} - \frac{n-\sum x_i}{1-p}$$ $$\Rightarrow (1-p)\sum x_i = p(n-\sum x_i)$$ $$\Rightarrow \sum x_i = pn$$ $$\Rightarrow \frac{1}{n}\sum x_i = p$$ giving us the candidate estimator $\bar{X}$ for $p$. Checking that it is a maximum using the second derivative: $$\frac{d^2}{dp^2}\log L(p) = -\left(\frac{\sum x_i}{p^2} + \frac{n - \sum x_i}{(1 - p)^2}\right)$$ Both elements in the sum are positive, so the second derivative is negative everywhere. $\bar{X}$ is the MLE for $p$.
    
    c. (1 point) We already know that $\bar{X}$ has expectation equal to the population mean. Since this is $p$ we know that $\bar{X}$ is unbiased. We also know that it is consistent due to the Law of Large Numbers. Since it is an unbiased estimator of $p$ the MSE is equal to its variance, which is the population variance divided by the sample size. $$\text{Var}(X_i) = E[X_i^2] - p^2 = 0^2(1-p) + 1^2(p) - p^2 = 1-p^2 = p(1-p)$$ $$\Rightarrow MSE(\bar{X}) = \text{Var}(\bar{X}) = \frac{p(1-p)}{n}.$$ We can also show consistency by noting that the MSE approaches zero as $n \to \infty$.
    
2. 
    a. (1/2 point) $$E[\hat{\mu_1}] = E\left[\frac{X_1 + X_2 + X_3}{3}\right] = \frac{1}{3}(E[X_1] + E[X_2] + E[X_3]) = \frac{1}{3}{3\mu} = \mu$$ So this estimator is unbiased. The MSE is then equal to its variance: $$MSE(\hat{\mu_1}) = \text{Var}(\hat{\mu_1}) = \text{Var}\left(\frac{X_1 + X_2 + X_3}{3}\right) = \frac{1}{3^2}\text{Var}(X_1 + X_2 + X_3) = \frac{1}{9}(2+2+2) = \frac{2}{3}.$$
    
    b. (1/2 point) Using similar calculations to above we get $$E[\hat{\mu_2}] =  E\left[\frac{2X_1 + X_2 + 5X_3}{8}\right] = \frac{1}{8}(2E[X_1] + E[X_2] + 5E[X_3]) = \frac{1}{8}{8\mu} = \mu$$ So this estimator is also unbiased with MSE equal to its variance. $$MSE(\hat{\mu_2}) = \text{Var}(\hat{\mu_2}) = \text{Var}\left(\frac{2X_1 + X_2 + 5X_3}{8}\right) = \frac{1}{8^2}\left(2^2\text{Var}(X_1) + \text{Var}(X_2) + 5^2\text{Var}(X_3)\right) = \frac{60}{64} = \frac{15}{16}.$$ This has a higher MSE than the first estimator, so we would prefer to use $\hat{\mu_1}$ out of the two if we were basing the decision on MSE.

3.  
    a. (1 point) We reject the null hypothesis falsely (type I error) if we flip three tails when the coin is actually fair. $\alpha = (1/2)^3 = 1/8$.

    b. (1 point) We commit type II error if the alternative hypothesis is true but we do not reject the null hypothesis. This means the true $p$ is 0.2 but we flip at least one head. This is one minus the probability of flipping all tails when the probability of heads is $0.2$, so it is $1 - (0.8)^3 = 0.488$.
  
    c. (1 point) $$P(\text{type I error}) = P(Y \leq 1|p=0.5) = P(Y = 0) + P(Y = 1) = (0.5)^5 + 5(0.5)^5 = 0.1875$$
    
4.

    a. (1/2 point) Since we are assuming that the underlying population is normally distributed, with a sample size too small for CLT approximations, we can use $t$ and $\chi^2$ pivots to construct the interval estimates for $\mu$ and $\sigma^2$, respectively. The relevant quantiles (using $\alpha$ to represent the upper tail): $t_{\alpha/2, n-1} = t_{0.025, 17} = 2.11$, $\chi^2_{1-\alpha/2, n-1} = \chi^2_{0.925, 17} = 7.56418$, $\chi^2_{\alpha/2, n-1} = \chi^2_{0.025, 17} = 30.191$. Estimate for $\mu$: $$2.27 \pm 2.11(\sqrt{1.02/18}) = (1.76772, 2.77228)$$ 
    
    b. (1/2 point) Estimate for $\sigma^2$: $$\left(\frac{(17)(1.02)}{30.191}, \frac{(17)1.02}{7.56418}\right) = (0.5743433,2.292383)$$ The statement is **false**. 
  
    b. (1 point) We use a one-sample $t$ test here, with test statistic $$T = \frac{\bar{X} - \mu_0}{S/\sqrt{n}}.$$ Plugging in our observed values we get $T = -3.066613$. Since this statistic has a $t$ distribution with 17 degrees of freedom if $H_0$ is true, to have level of significance $\alpha = 0.1$ we should reject the null hypothesis if $|T| > |t_{\alpha/2, n-1}| = |t_{0.05, 17}| = 1.74$. In this case we would **reject** the null hypothesis. Since the test statistic falls in the rejection region for our $\alpha$, we already know that its p-value must be less than $\alpha$. 
  
  
    c. (1 point) We use a chi-square test here, with test statistic $$\chi^2 = \frac{(n-1)S^2}{\sigma_0^2}.$$ Plugging in our observed values we get $\chi^2 = 4.335$. This statistic has a $\chi^2$ distribution with 17 degrees of freedom if $H_0$ is true, so to have level of significance $\alpha = 0.1$ we should reject the null hypothesis if $\chi^2 < \chi^2_{0.1, 17} = 10.0852$. Here we are using $0.1$ to denote the area under the curve to the left of the quantile. In this case we **reject** the null hypothesis. The statement in the question is **false**. 
  


  
  
  
  