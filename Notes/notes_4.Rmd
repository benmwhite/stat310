---
title: "Lecture Notes"
author: "Ben White"
date: "1/24/2018"
output: pdf_document
---

#### Conditional Probability (cont.)

An additional Bayes Rule and Law of Total Probability example.

**Ex:** A box contains two white balls and two black balls. A number is randomly chosen from the set $\{1, 2, 3, 4\}$, and that many balls are randomly drawn from the box. Suppose this experiment is performed and we are given the information that at least one white ball had been picked from the box. What is the probability that exactly one/two/three/four balls were drawn from the box given this observation?

 Let $D_i$ represent the event in which we draw $i$ balls. Since each of these events is equally likely, we have $$P(D_1) = P(D_2) = P(D_3) = P(D_4) = \frac{1}{4}.$$ Let $W$ represent the event in which we draw at least one white ball. When we draw one ball the probability of it being white is $1/2$, since two out of the four balls are white. $$P(W|D_1) = \frac{1}{2}.$$ When we draw two balls the probability of getting at least one white ball is one minus the probability that both balls are black (complementary events). The probability of drawing two black balls is $1/2 \times 1/3 = 1/6$ (first draw, then second draw). $$P(W | D_2) = 1 - \frac{1}{2}\times\frac{1}{3} = \frac{5}{6}$$ When we draw three or four balls we are guaranteed to get at least one white ball: $$P(W|D_3) = P(W|D_4) = 1$$ We can find $P(W)$ using the Law of Total Probability: $$P(W) = \sum_{i=1}^4 P(W|D_i)P(D_i) = (1/4)(1/2 + 5/6 + 1 + 1) = 5/6.$$ Applying Bayes' Rule to get the conditional probabilities: $$P(D_1|W) = \frac{P(W|D_1)P(D_1)}{P(W)} = \frac{(1/2)(1/4)}{5/6} = \frac{3}{20}$$ $$P(D_2|W) = \frac{P(W|D_2)P(D_2)}{P(W)} = \frac{(5/6)(1/4)}{5/6} = \frac{1}{4}$$ $$P(D_3|W) = \frac{P(W|D_3)P(D_3)}{P(W)} = \frac{1(1/4)}{5/6} = \frac{3}{10}$$  $$P(D_4|W) = \frac{P(W|D_4)P(D_4)}{P(W)} = \frac{1(1/4)}{5/6} = \frac{3}{10}$$ 

#### Random Variables

So far we have been discussing probability in terms of events and outcomes in a sample space, however it is often the case that will focus on some numerical summary of the experiment (e.g. number of heads, total of the dice, proportion of voters in a poll, etc.) Numeric functions of the outcomes are often simpler to work with and generalize more easily.

**Def**: A *random variable* $X$ is a function defined on a sample space $S$ that associates a real number, $X(\omega) = x$ with each outcome $\omega \in S$.

Although all random variables are considered to be functions defined on $S$, we will often denote them without functional notation: $X$ rather than $X(\omega)$.

**Ex:**: Two balanced coins are tosed and face values are recorded. $S = \{HH, HT, TH, TT\}$. Define the random variable $X(s) = n$, where $n$ is the number of heads observed and $s$ is an outcome in $S$. We can see that 
$$
X(s) = 
\begin{cases}
0, &\mbox{ if } s = (TT) \\
1, &\mbox{ if } s \in \{HT, TH\} \\
2, &\mbox{ if } s = (HH)
\end{cases}
$$
While the strict definition of random variables makes no references to probabilities, we can see that any probability function defined on the original $S$ will be "induced" into the new space. In this example, $$P(X(s) = 0) = 1/4, \quad P(X(s) = 1) = 1/2, \quad P(X(s) = 2) = 1/4$$. We can verify that this still satisfies the conditions of the axioms of probability.

Notation: since we often supress the functional notation, you'll see general probabilistic statements like $P(X = x)$. Usually the random variables themselves are represented with upper case letters, while the observed values (outputs of the function from the original experiment sample space to the real numbers) are represented with lower case letters. 

There are two types of random variables: *discrete* and *continuous*. We will define discrete rvs first, then discuss continuous rvs after introducing more material.

**Def**: A random variable $X$ is said to be *discrete* if it can only assume a finite or countably infinite number of distinct values.

**Def**: The *probability mass function* (abbreviated pmf) of a discrete random variable is the function $f(x)$ such that $$f(x_i) = P(X = x_i), \quad i = 1,2,3,...$$ This function has the value zero for any input value $x$ such that $P(X = x) = 0$, that is it is only positive for a countable set of values. This set of "possible" values for a random variable is known as its "support". Note also that pmfs will always be non-negative, and additionally since $X$ must always take on some value in its support, $\sum_x f(x) = 1$ where the sum is over all the values of $x$ for which $f(x)> 0$.

**Def**: The *cumulative distribution function* (abbreviated cdf) of a random variable $X$ is the function $$F(x) = P(X \leq x).$$ Note that this is defined for general random variables, and takes as input the entire real number line.

**Ex**: Back to the two coin example. What are the pmf and cdf of $X$ in this case (number of heads)? We know that there are only three possible values that $X$ can take (0,1, and 2) so we only need to define our function for those three values (since it's zero for all other numbers). Using the probabilities we found previously: 

$$
f(x) = P(X = x) = 
\begin{cases}
1/4 &\mbox{ if } x = 0 \\
1/2 &\mbox{ if } x = 1 \\
1/2 &\mbox{ if } x = 2 \\
0 &\mbox{ else }
\end{cases}
$$

Remember we need to define the cdf for all real numbers.

$$F(x) = P(X \leq x) = 
\begin{cases}
0 &\mbox{ if } x < 0 \\
1/4 &\mbox{ if } 0 \leq x < 1 \\
3/4 &\mbox{ if } 1 \leq x < 2 \\
1 &\mbox{ if } x \geq 2
\end{cases}
$$

Next we introduce continuous random variables. These can take on an uncountably infinite set of possible values, generally over a real interval or union of intervals.

**Def**: Let $X$ be a random variable. Suppose that there exists a nonnegative real-valued function $f:R \to [0,\infty)$ such that for any interval $[a,b]$: $$P(X \in [a,b] = P(a \leq X \leq B) = \int_a^b f(t) dt.$$ Then $X$ is called a *continuous random variable*, and $f(x)$ is its *probability distribution function* (abbreviated pdf). 

For a given function $f$ to be a valid pdf, it needs to satisfy the following conditions: 

- $f(x) \geq 0$ for all values of $x$.

- $\int_{-\infty}^{\infty} f(t)dt = 1$.

Any function satisfying these conditions can define a continuous random variable!

The cdf of continuous random variables is defined in the same way, but we can relate it to the pdf via the Fundamental Theorem of Calculus. By the definition of the pdf, $$F(x) = P(X \leq x) = \int_{-\infty}^x f(t)dt$$ This means that $$\frac{d}{dx}F(x) = f(x)$$, and that $P(X \in [a,b]) = F(b) - F(a)$.

Since $P(a \leq X \leq b) = \int_a^b f(x)dx$, for any real number $a$ we have that $P(X = a) = \int_a^a f(x) dx = 0$. This also implies that $$P(a\leq X\leq b) = P(a < X \leq b) = P(a \leq X < b) = P(a < X < b).$$ In other words, point probabilites are all equal to zero for continous random varialbes, and probabilities over intervals are the same whether the intervals are open or closed.

**Properties of cdfs**
For any cdf $F(x)$ of a random variable $X$, the following are true

- $0 \leq F(x) \leq 1$

- $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} = 1$

- $F(x)$ is non-decreasing and right-continuous.

**Ex**: Let $f(x)$ be defined as follows: $$f(x) = \lambda x e^{-x}$$ for $x > 0$, and zero otherwise. For what value of $\lambda$ is $f(x)$ a valid pdf, and what would the cdf of that random variable be?

By inspection $f(x)$ is nonnegative, so we just need to find a value of $\lambda$ such that $$\lambda \int_{-\infty}^\infty f(x) dx = 1$$ Setting equal to one and solving the integral: $$1 = \int_{-\infty}^\infty f(x) dx = \int_0^\infty \lambda x e^{-x} dx$$ $$=\lambda \int_0^\infty x e^{-x} dx = \lambda\left[-xe^{-x}|_0^\infty +\int_0^\infty e^{-x} dx \right]$$ $$ = \lambda \left[0 - e^{-x}|_0^{\infty}\right]$$ $$= \lambda$$

So $\lambda = 1$ makes this function a valid pdf. 

Next we need to find the cdf. 

$$F(x) = P(X \leq x) = \int_{-\infty}^x f(t) dt = 
\begin{cases}
0 &\mbox{ if } x \leq 0\\
\int_0^x te^{-t} dt = 1 - (x+1)e^{-x} &\mbox{ if } x > 0
\end{cases}$$



