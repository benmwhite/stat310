---
title: "Solution 3"
author: "Ben White"
date: "2/6/2018"
output: pdf_document
---

1. (1 point) Remember the relationship between the cdf and pdf for continuous random variables: $$\frac{d}{dt}F(t) = f(t).$$ So to find the pdf of $T$ we just need to differentiate the cdf with respect to $t$. For $t<\gamma$ the pdf is zero (since we are taking the derivative of a constant at this point). For $t \geq \gamma$: $$\frac{d}{dt}F(t) = \frac{d}{dt} \left(1 - \text{exp}\left(-\frac{(t - \gamma)^\beta}{\alpha}\right)\right)$$ $$= -\frac{d}{dt} e^{-\frac{(t - \gamma)^\beta}{\alpha}} = -e^{-\frac{(t - \gamma)^\beta}{\alpha}}\frac{d}{dt}\left(-\frac{(t - \gamma)^\beta}{\alpha}\right)$$ $$=\frac{\beta}{\alpha} (t - \gamma)^{\beta-1}e^{-\frac{(t - \gamma)^\beta}{\alpha}}$$ We treat all the parameters as constants in this process.

1. Let $\mu = E[X]$. 

    a. (1/2 point) $Var(aX+b) = E[(aX + b - E[aX + b])^2] = E[(aX + b - (a\mu + b))^2] = E[a^2(X - \mu)^2] = a^2Var(X)$
    
    b. (1/2 point) $Var(X) = E[(X - \mu)^2] = E[X^2 - 2\mu X + \mu^2] = E[X^2] - 2\mu E[X] + \mu^2 = E[X^2] - \mu^2$


1. (1 point) The variance of Bernoulli$(p)$ random variables is $p(1-p)$. To find the value of $p$ that maximizes that expression we take the first derivative, set it to zero, and solve for $p$. Next to verify that it is a maximum and not a minimum we take the second derivative and check that it is negative at that point. $$\frac{d}{dp} p(1-p) = \frac{d}{dp} (p - p^2) = 1 - 2p$$ $$1-2p = 0$$ $$p = 1/2$$ So our candidate is $p=1/2$. Checking the second derivative: $$\frac{d^2}{dp^2} p(1-p) = \frac{d}{dp} (1 - 2p) = -2 < 0$$ So the variance is at its maximum when $p = 1/2$.

1. (1 point) The pmf of $Y$ is the function $f_Y(y) = P(Y = y)$. The support is $\{0, 1, 2, 3, 4, 5\}$ since those are all the possible values $Y$ can take. For any $y$ in the support, we get $Y=y$ when we observe $y$ successes and $5-y$ failures. So we need to find all possible sequences that give us that number of successes and failures, then add up their probabilities (since all the individual sequences are mutually exclusive outcomes). The probability of any single sequence of results giving us that number of successes and failures is $p^y(1-p)^{5-y}$, since the trials are all independent. Next we need to count the total number of sequences of that type: ${5 \choose y} = \frac{5!}{y!(5-y)!}$. So $$P(Y = y) = {5 \choose y}p^y(1-p)^{5-y} = \frac{5!}{y!(5-y)!}p^y(1-p)^{5-y}$$ If we have $n$ instead of $5$ the same logic holds, and $$P(Y = y) = {n \choose y}p^y(1-p)^{n-y},$$ for $y = 0,1,...,n$ This is known as the "binomial" distribution with parameters $n$ and $p$, denoted Binom$(n,p)$.

1. 
    a. (1/2 point) $$E[X] = \int_{-\infty}^\infty xf_X(x) dx = \frac{1}{2}\int_{-\infty}^\infty xe^{-|x|}$$ $$=\frac{1}{2}\left(\int_{-\infty}^0 xe^x dx + \int_0^\infty xe^{-x} dx\right)$$ $$=\frac{1}{2}\left(-\int_0^\infty xe^{-x}dx + \int_0^\infty xe^{-x} dx \right)$$ $$=0$$ after verifying that the integral converges. This can be done using integration by parts ($\int u dv = uv - \int v du$) with $u=x$, $dv = e^{-x}$, $du = dx$, $v = -e^{-x}$: $$\int_0^\infty x e^{-x}dx = x(-e^{-x})|_0^\infty - \int_0^\infty-e^{-x}dx$$ $$= 0 - (-e^{-x})|_0^{\infty} = -(0 - 1) = 1$$
    
    b. (1/2 point) Since this random variable has an expected value of zero, this simplifies to $Var(X) = E[X^2]$. $$Var(X) = E[X^2] = \int_{-\infty}^\infty x^2 f_X(x) dx$$ $$= \frac{1}{2}\left(\int_{-\infty}^0 x^2 e^{x}dx +\int_0^\infty x^2e^{-x} dx\right)$$ $$= \frac{1}{2}\left(\int_{0}^\infty x^2 e^{-x}dx +\int_0^\infty x^2e^{-x} dx\right)$$ $$= \int_0^\infty x^2 e^{-x} dx$$ Using integration by parts with $u=x^2$, $dv = e^{-x}$, $du = 2x dx$, $v = -e^{-x}$: $$Var(X) = E[X^2] = \int_0^\infty x^2 e^{-x} dx$$ $$= x^2(-e^{-x})|_0^\infty - \int_0^\infty -2xe^{-x}dx$$ $$=0 + 2\int_0^\infty xe^{-x}dx$$ $$=2(1) = 2$$ where we plug in our solution to the last integral from our work in part (a).
  
1. This is known as the Exponential$(\beta)$ distribution.
    a. (1 point) $$E[X] = \int_{-\infty}^\infty x f_X(x) dx = \int_0^\infty \frac{x}{\beta}e^{-x/\beta} dx$$ Substitute $u = x/\beta$, $du = (1/\beta) dx$, $dx = \beta du$ (the limits of integration will still be $0$ and $\infty$ with this substitution): $$E[X] = \int_0^\infty \frac{x}{\beta}e^{-x/\beta} dx$$ $$=\int_0^\infty ue^{-u}\beta du$$ $$= \beta$$ using the integration by parts solution from the previous problem and taking the constant $\beta$ out of the integral. Next, $Var(X) = E[X^2] - (E[X])^2$. $$E[X^2] = \int_{-\infty}^\infty x^2 f_X(x) dx = \int_0^\infty \frac{x^2}{\beta}e^{-x/\beta} dx$$ Using the same substitution as before and noting that $x = \beta u$: $$E[X^2] = \int_0^\infty \frac{x^2}{\beta}e^{-x/\beta} dx$$ $$=\int_0^\infty u^2\beta^2 e^{-u} du$$ $$= 2\beta^2$$ after pulling out the constant $\beta^2$ from the integral and using the result from the previous problem. So $$Var(X) = E[X^2] - (E[X])^2 = 2\beta^2 - \beta^2 = \beta^2$$ 
    
    b. (1 point) $$M_X(t) = E[e{tX}] = \int_{-\infty}^\infty e^{tx}f_X(x) dx$$ $$=\int_0^\infty e^{tx}\frac{1}{\beta} e^{-x/\beta} dx$$ $$= \frac{1}{\beta}\int_0^\infty \exp\left(-(\frac{1}{\beta}-t)x \right) dx$$ We can see that this intgral will only converge if $t < 1/\beta$, otherwise the exponent is positive. Since this still converges for values of $t$ a neighborhood around zero, the mgf still exists and can be used to calculate the moments of this random variable. For $t < 1/\beta$ we can make the substitution $u = (\frac{1}{\beta}-t)x$, $du = (\frac{1}{\beta}-t) dx$, $dx = (\frac{1}{\beta}-t)^{-1} du = \beta/(1 - \beta t) du$: $$M_X(t)= \frac{1}{\beta}\int_0^\infty \exp\left(-(\frac{1}{\beta}-t)x \right) dx$$ $$= \frac{1}{\beta}\int_0^\infty e^{-u} \beta/(1 - \beta t) du$$ $$=\frac{\beta}{\beta(1 - \beta t)} \int_0^\infty e^{-u} du$$ $$=\frac{1}{1- \beta t} (-e^{-u})|_0^\infty$$ $$=\frac{1}{1 - \beta t} (0 - (-1))$$ $$= \frac{1}{1 - \beta t}$$ Note that we can also get the same result by turning the integrand into a valid exponential pdf with parameter $(\frac{1}{\beta}-t)^{-1}$, taking care of the constants, and turning the integral into 1. Using the mgf to get the mean and variance (use the chain rule for differentiation): $$\frac{d}{dt} M_X(t) = \frac{d}{dt} (1 - \beta t)^{-1} = (-1)(1 - \beta t)^{-2}(-\beta) = \beta(1 - \beta t)^{-2}.$$ Plugging in $t=0$ leaves us with $$E[X] = \frac{d}{dt}M_X(0) = \beta (1 - \beta(0))^{-2} = \beta$$ Second derivative: $$\frac{d^2}{dt^2}M_X(t) = \frac{d}{dt} \beta(1 - \beta t)^{-2} = \beta(-2)(1 - \beta t)^{-3}(-\beta) = 2\beta^2(1 - \beta t)^{-3}$$ Getting $E[X^2]$: $$E[X^2] = \frac{d^2}{dt^2}M_X(0) = 2\beta^2(1 - \beta (0))^{-3} = 2\beta^2$$ Which give us $$Var(X) = E[X^2] - (E[X])^2 = 2\beta^2 - \beta^2 = \beta^2$$ verifying the results from part (a).
  
1. (1 point) The support of $X$ is $\{1, 2, 3, 4, 5, 6\}$, and the pmf is $f_X(x) = 1/6$ since we are assuming the die is balanced. $$E[X] = (1/6)\sum_{x=1}^6 x = 3.5$$. Finding the mgf: $$M_X(t) = E[e^{tX}] = (1/6)\sum_{x=1}^6 e^{tx}$$ Finding $E[X^2]$: $$\frac{d}{dt}M_X(t) = \frac{1}{6}\sum_{x=1}^6 x e^{tx}$$ $$\frac{d^2}{dt^2}M_X(t) = \frac{1}{6} \sum_{x = 1}^6 x^2 e^{tx}$$ At $t = 0$: $$E[X^2] = \frac{1}{6} \sum_{x = 1}^6 x^2 e^{(0)x} = \frac{1}{6}\sum_{x = 1}^n x^2 = \frac{91}{6}$$ Finding $Var(X)$: $$Var(X) = E[X^2] - E[X]^2 = \frac{91}{6} - \left(\frac{7}{2}\right)^2 = \frac{35}{12}$$

1. This is known as the Geometric$(p)$ distribution.
    a. (1 point) The support is $y = 0,1,2,...$. To observe $Y = y$, we need to have gone through $y$ failed Bernoulli trials before the very last successful trial. The probability of seeing $y$ failed trials is $(1-p)^y$, and the probability of seeing the last successful trial after those first $x$ ones is $p$, since they are all independent. For any given $y$, there is only one exact sequence of successes and failures that can result in $Y = y$, so the pmf is $$f_Y(y) = P(Y = y) = p(1 - p)^y.$$ 
    
    b. (1 point) Deriving mgf: $$M_Y(t) = E[e^{tY}] = \sum_{y=0}^\infty e^{ty}p(1-p)^y = p\sum_{y=0}^\infty ((1-p)e^t)^y$$ $$=p(\frac{1}{1 - (1-p)e^t}) = \frac{p}{1 - (1-p)e^t}$$ This only holds when $(1-p)e^t < 1$, i.e. when $t < -\log(1-p)$. Note that the mgf still exists in a neighborhood of 0. Using the mgf to get the expectation: $$\frac{d}{dt}M_Y(t) = \frac{-p}{(1 - (1-p)e^t)^2}(-(1-p)e^t)$$ $$E[Y] = \frac{d}{dt}M_Y(0) = \frac{-p}{(1 - (1-p))^2}(-(1-p))$$ $$= \frac{p(1-p)}{p^2} = \frac{1-p}{p} = \frac{1}{p} - 1$$ So the expected number of failures we observe before the first success is the reciprocal of the success probability, subtracting one. As a brief note, if we were counting the total number of trials needed rather than just the number of failures (adding the one last success), this random variable would be $X = Y+1$ with expectation $E[X] = E[Y+1] = E[Y] + 1 = 1/p$. In other words if the success probability was $p = 1/2$ then we'd expect to go through 2 trials to see the first success, if $p = 1/3$ we'd expect to go through 3 trials, etc. 
